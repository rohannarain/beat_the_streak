{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsapi\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = pd.read_csv(\"player_stats_7_23_19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits['player_got_hit'] = hits['player_got_hit'].apply(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 97.   ,  91.   ,  58.   , ...,   1.667,   1.   ,   1.   ],\n",
       "       [ 18.   ,   5.   ,   1.   , ...,   0.   ,   0.   ,   0.   ],\n",
       "       [ 11.   ,   2.   ,   3.   , ...,   0.   ,   0.   ,   1.   ],\n",
       "       ..., \n",
       "       [ 95.   ,  57.   ,  78.   , ...,   0.773,   0.5  ,   0.   ],\n",
       "       [ 64.   ,  67.   ,  33.   , ...,   1.4  ,   1.   ,   1.   ],\n",
       "       [  7.   ,   1.   ,   1.   , ...,   0.   ,   0.   ,   0.   ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(hits.iloc[:, 3:-1])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "        1.,  1.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,\n",
       "        1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  1.,  0.,\n",
       "        0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "        0.,  0.,  1.,  1.,  0.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "        0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  1.,\n",
       "        0.,  1.,  1.,  1.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  0.,\n",
       "        0.,  1.,  0.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,\n",
       "        1.,  1.,  1.,  1.,  1.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.,  1.,\n",
       "        1.,  0.,  1.,  0.,  1.,  1.,  1.,  0.,  1.,  1.,  0.,  1.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        1.,  1.,  1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,\n",
       "        0.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  0.,  0.,  1.,\n",
       "        0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  0.,  0.,  1.,  1.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "        1.,  0.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  0.,  1.,  1.,  0.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.array(hits.iloc[:, -1])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data, labels, test_size=0.2)\n",
    "data_train, data_val, labels_train, labels_val = train_test_split(data_train, labels_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54935622317596566"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45064377682403434"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='newton-cg', C=100).fit(data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81974248927038629"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logreg.predict(data_train) == labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74576271186440679"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logreg.predict(data_val) == labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now the fun begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "365\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(data)\n",
    "num_feats = len(np.transpose(data))\n",
    "print(num_rows)\n",
    "print(num_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert our arrays to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_t = torch.from_numpy(data_train).type(torch.FloatTensor)\n",
    "data_val_t = torch.from_numpy(data_val).type(torch.FloatTensor)\n",
    "data_test_t = torch.from_numpy(data_test).type(torch.FloatTensor)\n",
    "\n",
    "labels_train_t = torch.from_numpy(labels_train).type(torch.LongTensor)\n",
    "labels_val_t = torch.from_numpy(labels_val).type(torch.LongTensor)\n",
    "labels_test_t = torch.from_numpy(labels_test).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then begin training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_feats, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, len(data_train_t))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "    \n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        predictions = F.softmax(self.forward(x))\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = .001\n",
    "epochs = 100\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for i in range(epochs):\n",
    "    outputs = model.forward(data_train_t)\n",
    "    loss = criterion(outputs, labels_train_t)\n",
    "    losses.append(loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1382243037223816,\n",
       " 3.30961012840271,\n",
       " 0.45492711663246155,\n",
       " 2.5906989574432373,\n",
       " 2.3309059143066406,\n",
       " 0.6081636548042297,\n",
       " 0.35924047231674194,\n",
       " 1.2671750783920288,\n",
       " 1.5102070569992065,\n",
       " 0.9819403886795044,\n",
       " 0.2631914019584656,\n",
       " 0.39550989866256714,\n",
       " 0.8843850493431091,\n",
       " 0.8810398578643799,\n",
       " 0.426871120929718,\n",
       " 0.15440185368061066,\n",
       " 0.37233003973960876,\n",
       " 0.6030002236366272,\n",
       " 0.5761265754699707,\n",
       " 0.3525485396385193,\n",
       " 0.17228670418262482,\n",
       " 0.25729283690452576,\n",
       " 0.4322543740272522,\n",
       " 0.37130671739578247,\n",
       " 0.190165713429451,\n",
       " 0.15425419807434082,\n",
       " 0.24627089500427246,\n",
       " 0.3045112192630768,\n",
       " 0.2563822269439697,\n",
       " 0.16785204410552979,\n",
       " 0.16283361613750458,\n",
       " 0.23163925111293793,\n",
       " 0.25197094678878784,\n",
       " 0.19298873841762543,\n",
       " 0.14706218242645264,\n",
       " 0.16991952061653137,\n",
       " 0.20614689588546753,\n",
       " 0.19580954313278198,\n",
       " 0.15613102912902832,\n",
       " 0.1433539092540741,\n",
       " 0.1693648248910904,\n",
       " 0.18507878482341766,\n",
       " 0.16345655918121338,\n",
       " 0.14349928498268127,\n",
       " 0.15358342230319977,\n",
       " 0.16939890384674072,\n",
       " 0.16335593163967133,\n",
       " 0.14510604739189148,\n",
       " 0.14245997369289398,\n",
       " 0.15540573000907898,\n",
       " 0.15803147852420807,\n",
       " 0.14634117484092712,\n",
       " 0.14118754863739014,\n",
       " 0.14857976138591766,\n",
       " 0.15230512619018555,\n",
       " 0.1453927755355835,\n",
       " 0.1399516463279724,\n",
       " 0.14430342614650726,\n",
       " 0.1479204297065735,\n",
       " 0.14368398487567902,\n",
       " 0.13957467675209045,\n",
       " 0.14172491431236267,\n",
       " 0.14421336352825165,\n",
       " 0.14221195876598358,\n",
       " 0.1392873376607895,\n",
       " 0.1405825912952423,\n",
       " 0.14233355224132538,\n",
       " 0.1401907503604889,\n",
       " 0.13783782720565796,\n",
       " 0.13909535109996796,\n",
       " 0.14021918177604675,\n",
       " 0.13895048201084137,\n",
       " 0.13798408210277557,\n",
       " 0.13850589096546173,\n",
       " 0.13861146569252014,\n",
       " 0.13763943314552307,\n",
       " 0.13743188977241516,\n",
       " 0.13773316144943237,\n",
       " 0.13787733018398285,\n",
       " 0.137306809425354,\n",
       " 0.13724903762340546,\n",
       " 0.137533500790596,\n",
       " 0.1372169852256775,\n",
       " 0.13675352931022644,\n",
       " 0.13664253056049347,\n",
       " 0.1365886628627777,\n",
       " 0.13626831769943237,\n",
       " 0.13602226972579956,\n",
       " 0.13623647391796112,\n",
       " 0.13608689606189728,\n",
       " 0.1357375681400299,\n",
       " 0.13577188551425934,\n",
       " 0.13580501079559326,\n",
       " 0.13550736010074615,\n",
       " 0.1354655772447586,\n",
       " 0.13555803894996643,\n",
       " 0.13534989953041077,\n",
       " 0.1351473182439804,\n",
       " 0.1352013796567917,\n",
       " 0.13512177765369415]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94849785407725318"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "np.mean(predicted.numpy() == labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
